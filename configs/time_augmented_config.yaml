# Time-Augmented GHM Equity Training Configuration
# This config trains a policy π(c, τ) where the agent observes time-to-horizon

# ============================================================================
# DYNAMICS PARAMETERS
# ============================================================================
dynamics:
  # Cash flow parameters
  alpha: 0.18           # Mean cash flow rate

  # Growth and rates
  mu: 0.01              # Growth rate
  r: 0.03               # Interest rate
  lambda_: 0.02         # Carry cost

  # Volatility parameters
  sigma_A: 0.25         # Permanent shock volatility
  sigma_X: 0.12         # Transitory shock volatility
  rho: -0.2             # Correlation between shocks

  # State bounds
  c_max: 2.0            # Maximum cash reserves

  # Equity issuance costs
  p: 1.06               # Proportional cost
  phi: 0.002            # Fixed cost

  # Liquidation (FIXED: liquidation_value=0 is set in code)
  omega: 0.55           # Recovery rate (unused)

# ============================================================================
# ACTION SPACE
# ============================================================================
action_space:
  # Dividend bounds
  dividend_min: 0.0
  dividend_max: 10.0

  # Equity issuance bounds
  equity_min: 0.0
  equity_max: 0.5

  # Control spec parameters
  a_L_max: 10.0
  a_E_max: 0.5
  issuance_threshold: 0.05
  issuance_cost: 0.0

# ============================================================================
# ENVIRONMENT
# ============================================================================
environment:
  dt: 0.1               # Time step (larger for time-augmented)
  max_steps: 100        # Max steps (T / dt = 10.0 / 0.1 = 100)
  seed: null

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
network:
  # Policy network (for Monte Carlo)
  policy_hidden: [256, 256, 128]
  policy_activation: relu

  # Value network (baseline)
  value_hidden: [256, 256, 128]
  value_activation: relu

  # Actor-Critic (if used)
  hidden_dims: [256, 256, 128]
  shared_layers: 0

  # Policy parameters
  log_std_bounds: [-2.0, 0.5]
  mean_output_clipping: [-10.0, 10.0]

# ============================================================================
# TRAINING
# ============================================================================
training:
  # Time horizon
  dt: 0.1               # Time step
  T: 10.0               # Total horizon

  # TIME-AUGMENTED DYNAMICS (KEY FLAG!)
  use_time_augmented: true  # Enable 2D state (c, τ)

  # Training iterations
  n_iterations: 10000
  n_trajectories: 256    # Batch size

  # Learning rates
  lr_policy: 0.0003      # 3e-4
  lr_baseline: 0.001     # 1e-3
  lr: 0.0003             # For actor-critic

  # Optimization
  max_grad_norm: 1.0
  advantage_normalization: true
  entropy_weight: 0.01

  # Baseline
  use_baseline: true

# ============================================================================
# SOLVER
# ============================================================================
solver:
  # Solver type
  solver_type: monte_carlo

  # Actor-Critic specific (not used here)
  critic_loss: temporal_difference
  actor_loss: policy_gradient
  hjb_weight: 0.0

  # Parallel simulation
  use_parallel: false
  n_workers: null

  # Monte Carlo
  batch_size: 1000

# ============================================================================
# REWARD FUNCTION
# ============================================================================
reward:
  discount_rate: null    # Use r - mu
  issuance_cost: 0.1     # Lambda for (1+λ)·a_E term
  liquidation_rate: 0.0  # Set to 0 (equity holders get nothing)
  liquidation_flow: 0.0  # No liquidation value

# ============================================================================
# LOGGING
# ============================================================================
logging:
  log_dir: logs/ghm_time_augmented
  log_freq: 50
  eval_freq: 500
  ckpt_freq: 1000
  ckpt_dir: checkpoints/ghm_time_augmented

# ============================================================================
# MISC
# ============================================================================
misc:
  seed: 42
  device: cpu           # Change to "cuda" if GPU available
  resume: null
  experiment_name: null
