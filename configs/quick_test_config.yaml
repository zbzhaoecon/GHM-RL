# Quick Test Configuration
# Faster training for debugging and testing code changes

dynamics:
  alpha: 0.18
  mu: 0.01
  r: 0.03
  lambda_: 0.02
  sigma_A: 0.25
  sigma_X: 0.12
  rho: -0.2
  c_max: 2.0
  p: 1.06
  phi: 0.002
  omega: 0.55

action_space:
  dividend_min: 0.0
  dividend_max: 2.0
  equity_min: 0.0
  equity_max: 2.0
  a_L_max: 10.0
  a_E_max: 0.5
  issuance_threshold: 0.05
  issuance_cost: 0.0

environment:
  dt: 0.01
  max_steps: 1000
  seed: 42

network:
  # Smaller networks for faster training
  policy_hidden: [32, 32]
  value_hidden: [32, 32]
  hidden_dims: [64, 64]
  shared_layers: 0
  policy_activation: tanh
  value_activation: tanh
  log_std_bounds: [-5.0, 2.0]
  mean_output_clipping: [-10.0, 10.0]

training:
  dt: 0.01
  T: 10.0

  # REDUCED iterations for quick testing
  n_iterations: 1000  # baseline: 10000
  n_trajectories: 100  # baseline: 500

  lr_policy: 0.001  # Slightly higher LR
  lr_baseline: 0.003
  lr: 0.001

  max_grad_norm: 0.5
  advantage_normalization: true
  entropy_weight: 0.05
  action_reg_weight: 0.01
  use_baseline: true

solver:
  solver_type: monte_carlo
  critic_loss: mc+hjb
  actor_loss: pathwise
  hjb_weight: 0.1
  use_parallel: false
  n_workers: null
  batch_size: 100  # Smaller batch

reward:
  discount_rate: null
  issuance_cost: 0.0
  liquidation_rate: 1.0
  liquidation_flow: 0.0

logging:
  log_dir: runs/quick_test
  log_freq: 50  # More frequent logging
  eval_freq: 200
  ckpt_freq: 500
  ckpt_dir: checkpoints/quick_test

misc:
  seed: 42
  device: cpu
  resume: null
  experiment_name: quick_test
