# Time-Augmented GHM Equity Training Configuration WITH SPARSE REWARDS
# This config trains a policy π(c, τ) using sparse (trajectory-level) rewards
# to reduce gradient variance and improve stability

# ============================================================================
# DYNAMICS PARAMETERS
# ============================================================================
dynamics:
  # Cash flow parameters
  alpha: 0.18           # Mean cash flow rate

  # Growth and rates
  mu: 0.01              # Growth rate
  r: 0.03               # Interest rate
  lambda_: 0.02         # Carry cost

  # Volatility parameters
  sigma_A: 0.25         # Permanent shock volatility
  sigma_X: 0.12         # Transitory shock volatility
  rho: -0.2             # Correlation between shocks

  # State bounds
  c_max: 2.0            # Maximum cash reserves

  # Equity issuance costs
  p: 1.06               # Proportional cost
  phi: 0.002            # Fixed cost

  # Liquidation
  omega: 0.55           # Recovery rate

# ============================================================================
# ACTION SPACE
# ============================================================================
action_space:
  # Dividend bounds
  dividend_min: 0.0
  dividend_max: 10.0

  # Equity issuance bounds
  equity_min: 0.0
  equity_max: 0.5

  # Control spec parameters
  a_L_max: 10.0
  a_E_max: 0.5
  issuance_threshold: 0.05
  issuance_cost: 0.0

# ============================================================================
# ENVIRONMENT
# ============================================================================
environment:
  dt: 0.1               # Time step
  max_steps: 100        # Max steps (T / dt = 10.0 / 0.1 = 100)
  seed: null

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
network:
  # Actor-Critic architecture (used when solver_type=actor_critic)
  hidden_dims: [64, 64, 32]    # Hidden dims for ActorCritic (smaller network)
  shared_layers: 0             # Number of shared layers (0=no sharing)

  # Policy network (for Monte Carlo)
  policy_hidden: [256, 256, 128]
  policy_activation: relu

  # Value network (baseline)
  value_hidden: [256, 256, 128]
  value_activation: relu

  # Policy parameters
  log_std_bounds: [-2.0, 0.5]
  mean_output_clipping: [-10.0, 10.0]

  # Distribution type for action sampling
  # Options:
  #   - tanh_normal: Gaussian with tanh squashing (default, good general purpose)
  #   - beta: Beta distribution (natural for [0, max], can represent zero naturally)
  #   - log_normal: Log-space Gaussian (for actions spanning orders of magnitude)
  # WARNING: Beta distribution has issues with pathwise gradients! Use tanh_normal for actor_critic
  distribution_type: tanh_normal  # CHANGED: Use tanh_normal for actor_critic pathwise gradients

# ============================================================================
# TRAINING
# ============================================================================
training:
  # Time horizon
  dt: 0.1               # Time step
  T: 10.0               # Total horizon

  # TIME-AUGMENTED DYNAMICS (KEY FLAG!)
  use_time_augmented: true  # Enable 2D state (c, τ)

  # SPARSE REWARDS (KEY FLAG!)
  use_sparse_rewards: true  # Compute trajectory return directly (reduces variance)

  # Training iterations
  n_iterations: 500     # Shorter test run
  n_trajectories: 512   # Increased from 256 to compensate for higher variance without baseline

  # Learning rates
  lr_policy: 0.0003     # 3e-4
  lr_baseline: 0.0001   # 1e-4 (FIXED: 3x slower than policy to prevent baseline overfitting)
  lr: 0.0003            # For actor-critic (single optimizer for both actor and critic)

  # Optimization
  max_grad_norm: 1.0
  advantage_normalization: false  # DISABLED: normalization destroys signal when all returns are small
  entropy_weight: 0.01

  # Baseline
  use_baseline: false  # DISABLED: baseline converges too fast, killing gradient signal

# ============================================================================
# SOLVER
# ============================================================================
solver:
  # Solver type: "monte_carlo" or "actor_critic"
  solver_type: actor_critic  # CHANGED: Use actor-critic for pathwise gradients

  # Actor-Critic parameters (used when solver_type=actor_critic)
  critic_loss: mc+hjb   # Options: mc, td, hjb, mc+hjb, mc+td, td+hjb
  actor_loss: pathwise  # Options: pathwise, reinforce
  hjb_weight: 0.1       # Weight for HJB regularization
  use_parallel: true    # Enable parallel simulation for speed
  n_workers: 8          # Number of parallel workers

  # Monte Carlo parameters (used when solver_type=monte_carlo)
  batch_size: 1000

# ============================================================================
# REWARD FUNCTION
# ============================================================================
reward:
  discount_rate: null    # Use r - mu
  issuance_cost: 0.06    # p - 1 = 1.06 - 1 (proportional cost, approximates (p-1)/p for p≈1)
  liquidation_rate: 0.0  # Set to 0 (equity holders get nothing)
  liquidation_flow: 0.0  # No liquidation value
  fixed_cost: 0.002      # φ = 0.002 (fixed cost per equity issuance)

# ============================================================================
# LOGGING
# ============================================================================
logging:
  log_dir: logs/ghm_time_augmented_sparse
  log_freq: 50
  eval_freq: 50
  ckpt_freq: 1000
  ckpt_dir: checkpoints/ghm_time_augmented_sparse

# ============================================================================
# MISC
# ============================================================================
misc:
  seed: 42
  device: cpu           # Change to "cuda" if GPU available
  resume: null
  experiment_name: sparse_rewards_test
