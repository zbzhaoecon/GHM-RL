# Default GHM-RL Configuration
# This file contains all configurable parameters for training and evaluation

# ============================================================================
# DYNAMICS PARAMETERS
# ============================================================================
# GHM model dynamics following Gromb-Huang-Mehrotra (2024)
dynamics:
  # Cash flow parameters
  alpha: 0.18           # Mean cash flow rate (expected earnings)

  # Growth and discount rates
  mu: 0.01              # Growth rate of the firm
  r: 0.03               # Risk-free interest rate
  lambda_: 0.02         # Carry cost rate

  # Volatility parameters
  sigma_A: 0.25         # Permanent shock volatility (affects firm value)
  sigma_X: 0.12         # Transitory shock volatility (affects cash flow)
  rho: -0.2             # Correlation between shocks

  # State space bounds
  c_max: 2.0            # Maximum cash/earnings ratio

  # Equity issuance costs (Table 1 in paper)
  p: 1.06               # Proportional cost of equity issuance (per dollar raised)
  phi: 0.002            # Fixed cost of equity issuance (as fraction of earnings)

  # Liquidation parameters
  omega: 0.55           # Liquidation recovery rate (fraction of firm value)

# ============================================================================
# ACTION SPACE CONFIGURATION
# ============================================================================
# Define bounds and constraints for actions (dividend, equity issuance)
action_space:
  # Dividend action bounds
  dividend_min: 0.0     # Minimum dividend rate
  dividend_max: 2.0     # Maximum dividend rate

  # Equity issuance action bounds
  equity_min: 0.0       # Minimum equity issuance
  equity_max: 2.0       # Maximum equity issuance

  # Control specification parameters (legacy compatibility)
  a_L_max: 10.0         # Maximum dividend rate (alternative specification)
  a_E_max: 0.5          # Maximum equity issuance rate (alternative specification)
  issuance_threshold: 0.05  # Minimum meaningful issuance (as fraction)
  issuance_cost: 0.0    # Additional issuance cost parameter

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
# Simulation environment setup
environment:
  dt: 0.01              # Time discretization step size
  max_steps: 1000       # Maximum steps per episode (horizon = max_steps * dt)
  seed: null            # Random seed for environment (null = random)

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
# Neural network architecture for policy and value functions
network:
  # Policy network (actor)
  policy_hidden: [64, 64]           # Hidden layer sizes for policy network
  policy_activation: tanh           # Activation function: tanh, relu, softplus

  # Value network (critic/baseline)
  value_hidden: [64, 64]            # Hidden layer sizes for value network
  value_activation: tanh            # Activation function: tanh, relu, softplus

  # Actor-Critic shared architecture
  hidden_dims: [256, 256]           # Hidden dimensions for actor-critic
  shared_layers: 0                  # Number of shared layers (0 = no sharing)

  # Policy output parameters
  log_std_bounds: [-5.0, 2.0]       # Bounds for log standard deviation
  mean_output_clipping: [-10.0, 10.0]  # Clipping bounds for policy mean output

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
# Training loop and optimization parameters
training:
  # Time horizon
  dt: 0.01              # Time step for training episodes
  T: 10.0               # Episode horizon (in time units)

  # Training iterations
  n_iterations: 10000   # Number of training iterations
  n_trajectories: 500   # Trajectories to sample per iteration

  # Optimization - separate learning rates
  lr_policy: 0.0003     # Policy network learning rate (3e-4)
  lr_baseline: 0.001    # Baseline/value network learning rate (1e-3)
  lr: 0.0003            # Combined learning rate for actor-critic (3e-4)
  max_grad_norm: 0.5    # Gradient clipping threshold

  # Regularization
  advantage_normalization: true    # Normalize advantages for variance reduction
  entropy_weight: 0.05             # Entropy regularization weight
  action_reg_weight: 0.01          # L2 regularization on action magnitude

  # Baseline settings
  use_baseline: true    # Use value function baseline for variance reduction

# ============================================================================
# SOLVER CONFIGURATION
# ============================================================================
# Algorithm-specific parameters
solver:
  # Solver type: "monte_carlo" or "actor_critic"
  solver_type: monte_carlo

  # Actor-Critic specific parameters
  critic_loss: mc+hjb   # Critic loss type: mc, td, mc+td, mc+hjb, td+hjb
  actor_loss: pathwise  # Actor loss type: pathwise, reinforce
  hjb_weight: 0.1       # Weight for HJB (Hamilton-Jacobi-Bellman) loss term

  # Parallel simulation
  use_parallel: false   # Enable parallel trajectory simulation
  n_workers: null       # Number of workers (null = auto-detect CPU count)

  # Monte Carlo specific
  batch_size: 1000      # Batch size for initial state sampling

# ============================================================================
# REWARD FUNCTION
# ============================================================================
# Reward function parameters
reward:
  # Discount rate (typically r - mu, computed from dynamics if null)
  discount_rate: null   # Override discount rate (null = use r - mu)

  # Costs and penalties
  issuance_cost: 0.0    # Additional multiplicative issuance cost

  # Liquidation/bankruptcy
  liquidation_rate: 1.0   # Liquidation rate multiplier
  liquidation_flow: 0.0   # Expected cash flow after liquidation

# ============================================================================
# LOGGING AND CHECKPOINTING
# ============================================================================
# Experiment tracking and model checkpointing
logging:
  log_dir: runs/ghm_rl          # TensorBoard log directory
  log_freq: 100                 # Logging frequency (iterations)
  eval_freq: 1000               # Evaluation frequency (iterations)
  ckpt_freq: 5000               # Checkpoint saving frequency (iterations)
  ckpt_dir: checkpoints/ghm_rl  # Checkpoint directory

# ============================================================================
# MISCELLANEOUS
# ============================================================================
# General settings
misc:
  seed: 123             # Global random seed for reproducibility
  device: cpu           # Device for computation: cpu, cuda
  resume: null          # Path to checkpoint to resume from (null = start fresh)
  experiment_name: null # Optional experiment name for logging
