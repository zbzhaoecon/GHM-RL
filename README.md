# MacroRL: Model-Based RL for Continuous-Time Finance

[![Documentation Status](https://readthedocs.org/projects/ghm-rl/badge/?version=latest)](https://macrorl.readthedocs.io/en/latest/?badge=latest)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-red.svg)](https://pytorch.org/)

A Python library for solving continuous-time corporate finance models using **model-based reinforcement learning**. This project implements the GHM (GÃ¢rleanu-Hackbarth-Morellec) equity management model using known dynamics to achieve superior sample efficiency and convergence.

ðŸ“– **[Full Documentation](https://macrorl.readthedocs.io/)** | ðŸš€ **[Quick Start](#quick-start)** | ðŸ“š **[Tutorials](https://macrorl.readthedocs.io/en/latest/tutorials/index.html)** | ðŸ“– **[API Reference](https://macrorl.readthedocs.io/en/latest/api/index.html)**

## What Changed: From Model-Free to Model-Based

### Why Pivot to Model-Based?

The GHM model gives us **exact knowledge of the dynamics**:
```
dc = Î¼(c)dt + Ïƒ(c)dW
```

where drift and diffusion are **known closed-form functions**. Model-free RL (PPO, SAC) ignores this and tries to learn optimal behavior purely from trial-and-error. Model-based RL exploits known dynamics to:

1. **Simulate freely**: Generate unlimited trajectories without environment interaction
2. **Explore completely**: Sample any initial state, not just reachable ones
3. **Reduce variance**: Use exact gradients (pathwise) instead of REINFORCE estimates
4. **Validate rigorously**: Check solutions against HJB equation

### Three Model-Based Approaches

| Method | Gradient Type | Key Idea | Best For |
|--------|---------------|----------|----------|
| **Pathwise Gradient** (Recommended) | Exact via chain rule | Backprop through differentiable simulation | Most use cases |
| Monte Carlo PG | REINFORCE with unlimited samples | Free simulation reduces variance | Baselines, comparison |
| Deep Galerkin Method | PDE residual minimization | Directly solve HJB equation | Advanced, validation |

---

## Architecture Overview

```
macro_rl/
â”œâ”€â”€ core/                  # Foundational abstractions
â”‚   â”œâ”€â”€ state_space.py     # State space representation
â”‚   â””â”€â”€ params.py          # Parameter management
â”‚
â”œâ”€â”€ dynamics/              # Continuous-time models (UNCHANGED - verified correct)
â”‚   â”œâ”€â”€ base.py            # ContinuousTimeDynamics interface
â”‚   â””â”€â”€ ghm_equity.py      # GHM 1D model (drift, diffusion, parameters)
â”‚
â”œâ”€â”€ simulation/            # NEW: SDE simulation engines
â”‚   â”œâ”€â”€ sde.py             # Numerical integration (Euler-Maruyama)
â”‚   â”œâ”€â”€ trajectory.py      # Trajectory generation for Monte Carlo
â”‚   â””â”€â”€ differentiable.py  # Differentiable simulation for pathwise gradients
â”‚
â”œâ”€â”€ control/               # NEW: Control specifications (TWO controls, not one!)
â”‚   â”œâ”€â”€ base.py            # ControlSpec interface
â”‚   â”œâ”€â”€ ghm_control.py     # GHM two-control spec (dividend + equity issuance)
â”‚   â””â”€â”€ masking.py         # Action masking utilities
â”‚
â”œâ”€â”€ rewards/               # NEW: Objective functions
â”‚   â”œâ”€â”€ base.py            # RewardFunction interface
â”‚   â”œâ”€â”€ ghm_rewards.py     # GHM net payout: dividends - dilution cost
â”‚   â””â”€â”€ terminal.py        # Terminal value specifications
â”‚
â”œâ”€â”€ policies/              # NEW: Policy representations
â”‚   â”œâ”€â”€ base.py            # Policy interface
â”‚   â”œâ”€â”€ neural.py          # Gaussian and deterministic policies
â”‚   â”œâ”€â”€ barrier.py         # Barrier/threshold policies (baselines)
â”‚   â””â”€â”€ tabular.py         # Grid-based policies (debugging)
â”‚
â”œâ”€â”€ values/                # NEW: Value function representations
â”‚   â”œâ”€â”€ base.py            # ValueFunction interface
â”‚   â”œâ”€â”€ neural.py          # Neural value networks (with autograd support)
â”‚   â””â”€â”€ analytical.py      # Analytical solutions (when known)
â”‚
â”œâ”€â”€ solvers/               # NEW: Model-based RL algorithms
â”‚   â”œâ”€â”€ base.py            # Solver interface
â”‚   â”œâ”€â”€ pathwise.py        # Pathwise gradient (RECOMMENDED)
â”‚   â”œâ”€â”€ monte_carlo.py     # Monte Carlo policy gradient
â”‚   â”œâ”€â”€ deep_galerkin.py   # Deep Galerkin Method (HJB-based)
â”‚   â””â”€â”€ actor_critic.py    # Model-based actor-critic
â”‚
â”œâ”€â”€ validation/            # NEW: Solution validation
â”‚   â”œâ”€â”€ hjb_residual.py    # HJB equation residual computation
â”‚   â”œâ”€â”€ boundary_conditions.py  # Smooth pasting, etc.
â”‚   â””â”€â”€ analytical_comparison.py  # Compare with known solutions
â”‚
â”œâ”€â”€ utils/                 # Utilities
â”‚   â”œâ”€â”€ autograd.py        # Gradient/Hessian computation
â”‚   â”œâ”€â”€ plotting.py        # Visualization
â”‚   â””â”€â”€ logging.py         # Training logs
â”‚
â”œâ”€â”€ envs/                  # Gymnasium environments (for model-free baselines)
â”‚   â””â”€â”€ ghm_equity.py      # (To be rewritten with two controls)
â”‚
â””â”€â”€ scripts/               # Training scripts
    â”œâ”€â”€ train_pathwise.py      # Pathwise gradient training (START HERE)
    â”œâ”€â”€ train_monte_carlo.py   # Monte Carlo training
    â””â”€â”€ train_dgm.py           # Deep Galerkin training
```

---

## Critical Fix: Two Controls, Not One

### Previous (Wrong) Formulation:
```python
# Single control: dividend rate only
action = policy(state)  # Scalar
reward = action  # Just dividend
```

**Problems**:
- No equity issuance mechanism
- Can't handle barrier/recapitalization
- Doesn't match Bolton et al. paper

### Correct Formulation:
```python
# Two controls: dividend + equity issuance
action = policy(state)  # (a_L, a_E)
a_L = action[0]  # Dividend rate (continuous)
a_E = action[1]  # Equity issuance (singular)

# Net payout to shareholders
reward = a_L * dt - (1 + Î») * a_E
#        ^^^^^^^^^   ^^^^^^^^^^^^^^^
#        dividend    dilution cost

# State evolution
dc = (Î± + c(r-Î»-Î¼) - a_L) * dt + a_E + Ïƒ(c) * dW
     \_________________/          \____/
         drift with div         issuance
```

**Key insight**: Shareholders care about **net payout** = dividends - equity dilution cost.

---

## Quick Start

### 1. Installation

```bash
git clone https://github.com/zbzhaoecon/GHM-RL.git
cd GHM-RL
pip install -e .
```


## Key Design Principles

### 1. Exploit Known Dynamics
Unlike model-free RL, we **know** the dynamics. This enables:
- Free simulation (no environment interaction)
- Exact gradients (pathwise derivatives)
- Direct PDE validation (HJB residual)

### 2. Separation of Concerns
```
Dynamics â†’ Simulation â†’ Policies/Values â†’ Solvers â†’ Validation
```
Each component is independently testable and reusable.

### 3. Batched Operations
All operations support `(batch, ...)` dimensions for GPU efficiency and Monte Carlo estimation.

### 4. PyTorch Throughout
- Automatic differentiation for gradients/Hessians
- GPU acceleration
- Consistent interface

---



## Documentation

Complete documentation is available at **[https://ghm-rl.readthedocs.io](https://ghm-rl.readthedocs.io)**

### Documentation Sections

- **[Getting Started](https://ghm-rl.readthedocs.io/en/latest/getting_started.html)**: Installation and quick start guide
- **[Tutorials](https://ghm-rl.readthedocs.io/en/latest/tutorials/index.html)**: Step-by-step tutorials
- **[API Reference](https://ghm-rl.readthedocs.io/en/latest/api/index.html)**: Complete API documentation
- **[Examples](https://ghm-rl.readthedocs.io/en/latest/examples.html)**: Working examples and use cases
- **[Core Concepts](https://ghm-rl.readthedocs.io/en/latest/concepts.html)**: Theoretical foundations

### Building Documentation Locally

```bash
cd docs
pip install -r requirements.txt
make html
```

View the built documentation:

```bash
# Linux/Mac
open build/html/index.html

# Or use Python's built-in server
cd build/html && python -m http.server
```

## Contributing

We welcome contributions! Please see the **[Contributing Guide](https://ghm-rl.readthedocs.io/en/latest/contributing.html)** for details on:

- Setting up development environment
- Code style and testing guidelines
- Submitting pull requests
- Documentation standards

---

## License

MIT

---

## Contact

For questions about implementation or research collaboration, please open an issue or contact the maintainers.
